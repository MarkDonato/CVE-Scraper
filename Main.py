# Goal: Use web scraping to grab all CVE's on cvedetails.com and build a local
#   database of these details. As the script keeps running it will add any
#   new entries to the CVE database and also alert about new CVE's to a
#   provided email.

# Imports
import sys
import string
from urllib.request import Request, urlopen
from bs4 import BeautifulSoup
import time


# Global Variables
vulnCount = 0;


# Switch to sql!
data_mama = open("CVE-Scraper.dat", "w+")


# Closes out the program cleanly
def close_scraper():
    print(vulnCount + "Vulnerabilities found.")
    print("\n\n\nGOODBYE")
    data_mama.close()
    quit()


def log_data(CVEID, CVEPage, CWEID, knownExploits, vulnClassification,
    publishDate, updateDate, score, accessGained, attackOrigin, complexity,
    authenticationRequired, confidentiality, integrity, availability):
    global vulnCount
    print("Logging cell data...")
    vulnCount = vulnCount + 1
    data_mama.write(CVEID + "\t | \t" +  CVEPage + "\t | \t" +  CWEID + "\t | \t" +  knownExploits + "\t | \t" +  vulnClassification + "\t | \t" +  publishDate + "\t | \t" +  updateDate + "\t | \t" +  score + "\t | \t" +  accessGained + "\t | \t" +  attackOrigin + "\t | \t" +  complexity + "\t | \t" +  authenticationRequired + "\t | \t" +  confidentiality + "\t | \t" +  integrity + "\t | \t" +  availability + "\n")

# does the heavy lifting of breaking down the CVE table and recording the scraped data
def record_data(pageURL):
    print("\tExtracting from: " + pageURL + "\n")
    pageSoup = BeautifulSoup(urlopen(Request(pageURL, headers={'User-Agent': 'Mozilla/5.0'})).read(), 'html.parser') # clean up later.
    pageTable = pageSoup.find('table', class_ = "searchresults sortable")
    for row in pageTable.findAll('tr', class_ = "srrowns"):
        print(row)
        print("=============\n\n")

        # data organization (don't judge) -> turn into array at some point
        CVEID = "NULL"
        CVEPage = "NULL"
        CWEID = "NULL"
        knownExploits = "NULL"
        vulnClassification = "NULL"
        publishDate = "NULL"
        updateDate = "NULL"
        score = "NULL"
        accessGained = "NULL"
        attackOrigin = "NULL"
        complexity = "NULL"
        authenticationRequired = "NULL"
        confidentiality = "NULL"
        integrity = "NULL"
        availability = "NULL"

        index = 0
        for cell in row.findAll('td'):
            print("<" + str(cell.next) + ">")
            # Push scraped cell data into organized variables
            if(index == 1):
                CVEPage = ("https://www.cvedetails.com" + (cell.find('a'))['href'])
                CVEID = cell.find('a').next
            if(index == 2):
                CWEID = str(cell.next).strip("\r\n\t")
            if(index == 3):
                knownExploits = str(cell.next).strip("\r\n\t")
            if(index == 4):
                vulnClassification = str(cell.next).strip("\r\n\t")
            if(index == 5):
                publishDate = str(cell.next).strip("\r\n\t")
            if(index == 6):
                updateDate = str(cell.next).strip("\r\n\t")
            if(index == 7):
                score = cell.find('div').next
            if(index == 8):
                accessGained = str(cell.next).strip("\r\n\t")
            if(index == 9):
                attackOrigin = str(cell.next).strip("\r\n\t")
            if(index == 10):
                complexity = str(cell.next).strip("\r\n\t")
            if(index == 11):
                authenticationRequired = str(cell.next).strip("\r\n\t")
            if(index == 12):
                confidentiality = str(cell.next).strip("\r\n\t")
            if(index == 13):
                integrity = str(cell.next).strip("\r\n\t")
            if(index == 14):
                availability = str(cell.next).strip("\r\n\t")

            print("---")
            index += 1

        # List all values gained from this row
        print("\n\n")
        print("===")
        print("CVE ID:\t\t\t\t" + CVEID)
        print("CVE Page:\t\t\t" + CVEPage)
        print("CWE ID:\t\t\t\t" + CWEID)
        print("Number of Exploits:\t\t\t" + knownExploits)
        print("Vulnerability Classification:\t" + vulnClassification)
        print("Publish Date:\t\t\t" + publishDate)
        print("Update Date:\t\t\t" + updateDate)
        print("CVSS Score:\t\t\t" + score)
        print("Access Gained:\t\t\t" + accessGained)
        print("Attack Origin:\t\t\t" + attackOrigin)
        print("Complexity:\t\t\t" + complexity)
        print("Authentication Required:\t" + authenticationRequired)
        print("Confidentiality:\t\t" + confidentiality)
        print("Integrity:\t\t\t" + integrity)
        print("Availability:\t\t\t" + availability)
        print("===\n\n")

        log_data(CVEID, CVEPage, CWEID, knownExploits, vulnClassification,
            publishDate, updateDate, score, accessGained, attackOrigin, complexity,
            authenticationRequired, confidentiality, integrity, availability)


###############################################################################
# MAIN
###############################################################################
def main(argv):
    print("\n==== CVE-Scraper ====\n\n")

    data_mama.write("CVE ID 	CWE ID 	# of Exploits 	Vulnerability Type(s) 	Publish Date 	Update Date 	Score 	Gained Access Level 	Access 	Complexity 	Authentication 	Conf. 	Integ. 	Avail. \n\n")

    # grab the browse-by-date page and throw it in beautifulSoup.
    catalogSoup=BeautifulSoup(urlopen(Request("https://www.cvedetails.com/browse-by-date.php", headers={'User-Agent': 'Mozilla/5.0'})).read(), 'html.parser') # clean up later.

    # Scrape the browse-by-date page to gather all of the different month's links
    catalogTable = catalogSoup.find('table', class_='stats')
    yearlyReports = []
    for row in catalogTable.findAll('th'):
        for year in row.findAll('a', href=True):
            print("Found year at: https://www.cvedetails.com" + year['href'] + "\n")
            yearlyReports.append("https://www.cvedetails.com" + year['href'])

    print("\n === Years discovered. Grabbing pages for each year ===\n\n")
    # discover the pages for each year and pass on those pages to be dissected
    for yearURL in yearlyReports:
        yearTableSoup = BeautifulSoup(urlopen(Request(yearURL, headers={'User-Agent': 'Mozilla/5.0'})).read(), 'html.parser') # clean up later.
        pageIndex = yearTableSoup.find('div', {'id':'pagingb'}, class_='paging')
        for page in pageIndex.findAll('a', href=True):
            pageURL = ("https://www.cvedetails.com" + page['href'])
            record_data(pageURL)

        data_mama.write("\n\n\n=====================================================\n\n\n")
        close_scrapper()

    close_scraper()


if __name__ == "__main__":
    main(sys.argv[1:])
