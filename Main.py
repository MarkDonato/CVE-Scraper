# Goal: Use web scraping to grab all CVE's on cvedetails.com and build a local
#   database of these details. As the script keeps running it will add any
#   new entries to the CVE database and also alert about new CVE's to a
#   provided email.

# Imports
import sys
import string
from urllib.request import Request, urlopen
from bs4 import BeautifulSoup
import time
from Database_functions import link_db, initialize_database,\
    insert_cve_into_database, close_database_functions
import datetime


# Global Variables
vulnCount = 0;
data_log = open("CVE-Scraper.dat", "w+") # volatile extra data storage [JSON]

# Error log writer
error_log = open("./Logs/main_log.log", "a+")


# Log errors to the file specified by error_log
def log_message(msg):
    timestamp = str(datetime.datetime.now())
    error_log.write(timestamp + ":\t" + msg + "\n")


# Closes out the program cleanly
def close_scraper(database_connection):
    log_message("closing main process safely")
    print(str(vulnCount) + " Vulnerabilities found.")
    print("\n\n\nGOODBYE")
    data_log.close()
    error_log.close()
    database_connection.close()
    close_database_functions()
    quit()


# Logs data  scraped outside of SQL as well as other misc. data points of
#   potential interest/use
def log_data(CVEID, CVEPage, CWEID, knownExploits, vulnClassification,
             publishDate, updateDate, score, accessGained, attackOrigin,
             complexity, authenticationRequired, confidentiality, integrity,
             availability):
    global vulnCount
    print("Logging cell data...")
    vulnCount = vulnCount + 1
    print("VULNERABILITIES FOUND: " + str(vulnCount))
    data_log.write('{\n\t"CVE ID":\"' + CVEID + '\",\n\t"CVE Page":\"' +
                   CVEPage + '\",\n\t"CWE ID":\"' + CWEID +
                   '\",\n\t"Known Exploits":\"' + knownExploits +
                   '\",\n\t"Vulnerability Classification":\"' +
                   vulnClassification + '\",\n\t"Publish Date":\"' + publishDate
                   + '\",\n\t"Update Date":\"' + updateDate +
                   '\",\n\t"Score":\"' + score + '\",\n\t"Access Gained":\"' +
                   accessGained + '\",\n\t"Attack Origin":\"' + attackOrigin +
                   '\",\n\t"Complexity":\"' + complexity +
                   '\",\n\t"Authentication Required":\"' +
                   authenticationRequired + '\",\n\t"Confidentiality":\"' +
                   confidentiality + '\",\n\t"Integrity":\"' + integrity +
                   '\",\n\t"Availability":\"' + availability + '\"\n}\n\n')

    print('{\n\t"CVE ID":\"' + CVEID + '\",\n\t"CVE Page":\"' +
          CVEPage + '\",\n\t"CWE ID":\"' + CWEID +
          '\",\n\t"Known Exploits":\"' + knownExploits +
          '\",\n\t"Vulnerability Classification":\"' +
           vulnClassification + '\",\n\t"Publish Date":\"' + publishDate
           + '\",\n\t"Update Date":\"' + updateDate +
           '\",\n\t"Score":\"' + score + '\",\n\t"Access Gained":\"' +
           accessGained + '\",\n\t"Attack Origin":\"' + attackOrigin +
           '\",\n\t"Complexity":\"' + complexity +
           '\",\n\t"Authentication Required":\"' +
           authenticationRequired + '\",\n\t"Confidentiality":\"' +
           confidentiality + '\",\n\t"Integrity":\"' + integrity +
           '\",\n\t"Availability":\"' + availability + '\"\n}\n\n')


# Sends data to SQL
def store_data(CVEID, CVEPage, CWEID, knownExploits, vulnClassification,
               publishDate, updateDate, score, accessGained, attackOrigin,
               complexity, authenticationRequired, confidentiality, integrity,
               availability, database_connection):
    print("STORING DATA TO SQL")
    with database_connection:
        cve_data = (CVEID, CVEPage, CWEID, knownExploits, vulnClassification,
                    publishDate, updateDate, score, accessGained, attackOrigin,
                    complexity, authenticationRequired, confidentiality,
                    integrity, availability);
        insert_cve_into_database(database_connection, cve_data)


# Does the heavy lifting of breaking down the CVE tables
def record_data(pageURL, database_connection):
    log_message("scrape extracting from: " + pageURL + "\n")
    pageSoup = BeautifulSoup(urlopen(Request(pageURL,
                             headers={'User-Agent': 'Mozilla/5.0'})).read(),
                             'html.parser')

    pageTable = pageSoup.find('table', class_ = "searchresults sortable")
    for row in pageTable.findAll('tr', class_ = "srrowns"):
        print(row)

        # Temp variables to hold data that will be stored.
        CVEID = "NULL"
        CVEPage = "NULL"
        CWEID = "NULL"
        knownExploits = "NULL"
        vulnClassification = "NULL"
        publishDate = "NULL"
        updateDate = "NULL"
        score = "NULL"
        accessGained = "NULL"
        attackOrigin = "NULL"
        complexity = "NULL"
        authenticationRequired = "NULL"
        confidentiality = "NULL"
        integrity = "NULL"
        availability = "NULL"

        index = 0
        for cell in row.findAll('td'):
            print("<" + str(cell.next) + ">")
            # Push scraped cell data into organized variables
            if(index == 1):
                CVEPage = ("https://www.cvedetails.com" +
                          (cell.find('a'))['href'])
                CVEID = cell.find('a').next
            if(index == 2):
                CWEID = str(cell.next).strip("\r\n\t")
            if(index == 3):
                knownExploits = str(cell.next).strip("\r\n\t")
            if(index == 4):
                vulnClassification = str(cell.next).strip("\r\n\t")
            if(index == 5):
                publishDate = str(cell.next).strip("\r\n\t")
            if(index == 6):
                updateDate = str(cell.next).strip("\r\n\t")
            if(index == 7):
                score = cell.find('div').next
            if(index == 8):
                accessGained = str(cell.next).strip("\r\n\t")
            if(index == 9):
                attackOrigin = str(cell.next).strip("\r\n\t")
            if(index == 10):
                complexity = str(cell.next).strip("\r\n\t")
            if(index == 11):
                authenticationRequired = str(cell.next).strip("\r\n\t")
            if(index == 12):
                confidentiality = str(cell.next).strip("\r\n\t")
            if(index == 13):
                integrity = str(cell.next).strip("\r\n\t")
            if(index == 14):
                availability = str(cell.next).strip("\r\n\t")

            print("---")
            index += 1

        # List all values gained from this row
        print("\n\n")
        print("===")
        print("CVE ID:\t\t\t\t" + CVEID)
        print("CVE Page:\t\t\t" + CVEPage)
        print("CWE ID:\t\t\t\t" + CWEID)
        print("Number of Exploits:\t\t\t" + knownExploits)
        print("Vulnerability Classification:\t" + vulnClassification)
        print("Publish Date:\t\t\t" + publishDate)
        print("Update Date:\t\t\t" + updateDate)
        print("CVSS Score:\t\t\t" + score)
        print("Access Gained:\t\t\t" + accessGained)
        print("Attack Origin:\t\t\t" + attackOrigin)
        print("Complexity:\t\t\t" + complexity)
        print("Authentication Required:\t" + authenticationRequired)
        print("Confidentiality:\t\t" + confidentiality)
        print("Integrity:\t\t\t" + integrity)
        print("Availability:\t\t\t" + availability)
        print("===\n\n")

        log_data(CVEID, CVEPage, CWEID, knownExploits, vulnClassification,
            publishDate, updateDate, score, accessGained, attackOrigin,
            complexity, authenticationRequired, confidentiality, integrity,
            availability)

        store_data(CVEID, CVEPage, CWEID, knownExploits, vulnClassification,
            publishDate, updateDate, score, accessGained, attackOrigin,
            complexity, authenticationRequired, confidentiality, integrity,
            availability, database_connection)


# Head function for scraping general cve data
def scrape_cve_data(database_connection):
    # grab the browse-by-date page and throw it in beautifulSoup.
    pageURL = "https://www.cvedetails.com/browse-by-date.php"
    log_message("Scrape starting up... root page: " + pageURL)
    catalogSoup=BeautifulSoup(urlopen(Request(pageURL,
                              headers={'User-Agent': 'Mozilla/5.0'})).read(),
                              'html.parser')

    # Scrape the browse-by-date page to gather all of the different month's links
    catalogTable = catalogSoup.find('table', class_='stats')
    yearlyReports = []
    for row in catalogTable.findAll('th'):
        for year in row.findAll('a', href=True):
            print("Found year at: https://www.cvedetails.com" + year['href'] + "\n")
            yearlyReports.append("https://www.cvedetails.com" + year['href'])

    print("\n === Years discovered. Grabbing pages for each year ===\n\n")

    # discover the pages for each year and pass on those pages to be dissected
    for yearURL in yearlyReports:
        yearTableSoup = BeautifulSoup(urlopen(Request(yearURL,
                                      headers={'User-Agent': 'Mozilla/5.0'}))\
                                      .read(), 'html.parser')

        pageIndex = yearTableSoup.find('div', {'id':'pagingb'}, class_='paging')
        for page in pageIndex.findAll('a', href=True):
            pageURL = ("https://www.cvedetails.com" + page['href'])
            record_data(pageURL, database_connection)


###############################################################################
# MAIN
###############################################################################
def main(argv):
    print("\n==== CVE-Scraper ====")
    print("==== Main.py ====\n")
    print("PYTHON VERSION:\t\t" + sys.version)
    log_message("CVE-Scraper Starting up...")

    # Database-Broker function calls for initialization

    # get everything related to the database ready to rumble
    database_name = "cve.db"
    database_connection = link_db(database_name)
    log_message("initializing database connection to " + database_name)
    initialize_database(database_connection)

    # close_scraper(database_connection)

    scrape_cve_data(database_connection)
    log_message("Scrape complete")


# Call Main
if __name__ == "__main__":
    main(sys.argv[1:])
